{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVZ65Sm0HeWAtSVDuUVdog",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DhimanTarafdar/AAA/blob/main/final_weakly_summery_ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Explain gradient descent in detail. What is the main idea behind it? How does it help in minimizing the loss function? Explain the role of learning rate and describe what happens if the learning rate is too high or too low."
      ],
      "metadata": {
        "id": "Cl2AovO_Bdqo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "Gradient Descent হলো একটি মৌলিক optimization algorithm যা আমাদের দেখায় কীভাবে মডেলের parameter গুলো adjust করতে হয় যাতে error কমানো যায়। নিচে এটি কীভাবে কাজ করে তার বিস্তারিত ব্যাখ্যা দেওয়া হলো।\n",
        "\n",
        "### 1. The Main Idea Behind Gradient Descent\n",
        "মূল ধারণা হলো **Weights ($W$)** এবং **Bias ($b$)** এর সর্বোত্তম মান খুঁজে বের করা যাতে মডেলের prediction যতটা সম্ভব সঠিক হয়। এটি একটি কম্পাসের মতো কাজ করে যা বলে দেয় কোন দিকে গেলে error সবচেয়ে কম হবে।\n",
        "\n",
        "* **The Process:** এটি বর্তমান অবস্থানে loss function এর gradient (slope) হিসাব করে।\n",
        "* **Direction:** যদি slope ধনাত্মক (positive) হয়, তাহলে আমরা বিপরীত দিকে যাই। যদি ঋণাত্মক (negative) হয়, তাহলে সামনে এগোই।\n",
        "* **Goal:** আমরা বারবার weight update করি যতক্ষণ না **Global Minimum** এ পৌঁছাই, যেখানে loss সর্বনিম্ন হয় এবং আমরা সর্বোত্তম class separation পাই।\n",
        "\n",
        "\n",
        "\n",
        "### 2. Minimizing the Loss Function (Mathematical Intuition)\n",
        "আমাদের মডেল কতটা ভুল করছে তা নির্ণয় করার জন্য আমরা **Mean Squared Error (MSE)** loss function ব্যবহার করি। একটি single prediction এর জন্য loss ($L$) হলো:\n",
        "\n",
        "$$L = (y - \\hat{y})^2$$\n",
        "\n",
        "যেহেতু $\\hat{y} = W \\cdot X + b$, তাই আমরা দেখতে পাচ্ছি loss আমাদের weight এর উপর নির্ভরশীল। এটিকে minimize করার জন্য আমরা loss কে weight ($W$) এর প্রতি partial derivative করি:\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial W} = \\frac{\\partial}{\\partial W} (y - (W \\cdot X + b))^2$$\n",
        "$$\\frac{\\partial L}{\\partial W} = 2(y - \\hat{y}) \\cdot (-X)$$\n",
        "\n",
        "**How it helps:**\n",
        "Derivative $\\frac{\\partial L}{\\partial W}$ আমাদের error hill এর \"slope\" জানায়। যখন error বেশি থাকে, তখন derivative বড় হয়, ফলে update বড় হয়। আর যখন আমরা সঠিক weight এর কাছাকাছি যাই, তখন $\\frac{\\partial L}{\\partial W}$ খুব ছোট হয়ে যায় (শূন্যের দিকে ধাবিত হয়)। এর ফলে মডেল ধীরে ধীরে স্থির হয়ে সর্বোত্তম মানে পৌঁছায়।\n",
        "\n",
        "### 3. The Weight Update Rule\n",
        "প্রতিটি iteration এ আমরা নিচের সূত্র ব্যবহার করে weight update করি:\n",
        "\n",
        "$$W_{new} = W_{old} - \\eta \\cdot \\frac{\\partial L}{\\partial W}$$\n",
        "\n",
        "এখানে $\\eta$ (Eta) হলো **Learning Rate**।\n",
        "\n",
        "### 4. The Role of Learning Rate ($\\eta$)\n",
        "Learning rate সবচেয়ে গুরুত্বপূর্ণ setting কারণ এটি নির্ধারণ করে আমরা minimum এর দিকে কত বড় \"step\" নেব।\n",
        "\n",
        "* **If the Learning Rate is too Small:**\n",
        "    * মডেল খুব ছোট ছোট step নেয়।\n",
        "    * **Result:** Training খুব ধীরে হয় এবং সিদ্ধান্তে পৌঁছাতে অনেক সময় লাগে। এমনকি global minimum এ পৌঁছানোর আগেই আটকে যেতে পারে।\n",
        "  \n",
        "* **If the Learning Rate is too High:**\n",
        "    * মডেল খুব বড় \"jump\" নেয়।\n",
        "    * **Result:** এটি global minimum এর উপর দিয়ে লাফিয়ে চলে যেতে পারে। সর্বোত্তম পয়েন্টে স্থির হওয়ার বদলে সামনে-পেছনে bounce করতে থাকে এবং loss বাড়তেও পারে।\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "OpvAttnTBlya"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kX8UlML5BfwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. What is forward propagation in an artificial neural network? Explain step by step how data moves from the input layer to the output layer."
      ],
      "metadata": {
        "id": "KE8k3cFOBy7p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "Forward propagation হলো সেই প্রক্রিয়া যার মাধ্যমে input data নেটওয়ার্কের বিভিন্ন layer অতিক্রম করে একটি prediction তৈরি করে। এটি হলো একটি neural network কীভাবে \"চিন্তা\" করে এবং input থেকে output ম্যাপ করে তার মৌলিক উপায়।\n",
        "\n",
        "### 1. The Core Concept\n",
        "একটি Artificial Neural Network (ANN) এ forward propagation মূলত একাধিক linear algebra operation এবং তার পরে non-linear transformation এর সমষ্টি। Data input layer থেকে শুরু করে hidden layer গুলো অতিক্রম করে শেষ পর্যন্ত output layer এ পৌঁছে আমাদের চূড়ান্ত ফলাফল ($\\hat{y}$) প্রদান করে।\n",
        "\n",
        "### 2. Step-by-Step Data Flow\n",
        "আমরা layer গুলোর মধ্যে data চলাচলকে নিচেরভাবে ব্যাখ্যা করতে পারি:\n",
        "\n",
        "* **From Input Layer ($a^{[0]}$) to Hidden Layer 1 ($a^{[1]}$):**\n",
        "    Input feature ($X$ বা $a^{[0]}$) weight ($W^{[1]}$) দিয়ে গুণ করা হয় এবং তার সাথে bias ($b^{[1]}$) যোগ করা হয়। এরপর এই ফলাফলকে একটি activation function যেমন **Sigmoid** ($\\sigma$) এর মাধ্যমে পাঠানো হয় যাতে non-linearity যুক্ত হয়।\n",
        "    $$z^{[1]} = W^{[1]} \\cdot a^{[0]} + b^{[1]}$$\n",
        "    $$a^{[1]} = \\sigma(z^{[1]})$$\n",
        "\n",
        "* **From Hidden Layer 1 ($a^{[1]}$) to Hidden Layer 2 ($a^{[2]}$):**\n",
        "    প্রথম hidden layer এর output দ্বিতীয় layer এর input হিসেবে কাজ করে।\n",
        "    $$z^{[2]} = W^{[2]} \\cdot a^{[1]} + b^{[2]}$$\n",
        "    $$a^{[2]} = \\sigma(z^{[2]})$$\n",
        "\n",
        "* **Generating the Final Output ($\\hat{y}$):**\n",
        "    এভাবে data output layer পর্যন্ত পৌঁছায়। যদি আমাদের 3টি layer থাকে, তাহলে চূড়ান্ত prediction ($\\hat{y}$) বা $a^{[3]}$ হিসাব করা হয়।\n",
        "\n",
        "\n",
        "\n",
        "### 3. The General Equation (Nested View)\n",
        "পুরো প্রক্রিয়াটি গাণিতিকভাবে দেখলে এটি একটি nested function এর মতো। একটি 3-layer network এর জন্য সামগ্রিক গণনাটি হবে:\n",
        "\n",
        "$$\\hat{y} = \\sigma(W^{[3]} \\cdot \\sigma(W^{[2]} \\cdot \\sigma(W^{[1]} \\cdot X + b^{[1]}) + b^{[2]}) + b^{[3]})$$\n",
        "\n",
        "### 4. Role of the Sigmoid Function\n",
        "Forward propagation চলাকালে **Sigmoid Function** অত্যন্ত গুরুত্বপূর্ণ ভূমিকা পালন করে।\n",
        "* **Non-linearity:** এটি না থাকলে আমাদের মডেল একটি সাধারণ linear regressor এ পরিণত হতো।\n",
        "* **Curved Boundaries:** যেমন আমি লক্ষ্য করেছি, sigmoid function মডেলকে decision boundary বাঁকাতে (bend) সাহায্য করে। এর ফলে curved boundary তৈরি হয়, যা জটিল pattern classify করতে সক্ষম যেখানে একটি সরল রেখা ব্যর্থ হয়।\n",
        "* **Output Range:** এটি যেকোনো input মানকে 0 থেকে 1 এর মধ্যে সীমাবদ্ধ (squash) করে, যা probability ভিত্তিক classification এর জন্য উপযুক্ত।\n",
        "\n",
        "\n",
        "\n",
        "### 5. Intuition\n",
        "প্রতিটি layer এর neuron পূর্বের layer থেকে signal গ্রহণ করে, একটি weighted sum গণনা করে এবং activation function এর মাধ্যমে সিদ্ধান্ত নেয় কতটুকু signal সামনে পাঠাবে। এইভাবে layer-by-layer তথ্য আদান-প্রদান করার প্রক্রিয়াকেই আমরা Forward Propagation বলি।\n",
        "\n",
        "---\n",
        "```\n"
      ],
      "metadata": {
        "id": "CDb4BDhvCQ-j"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a9nEaAP_B3aF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}