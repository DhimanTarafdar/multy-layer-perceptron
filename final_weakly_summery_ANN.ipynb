{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvWQz5GjLej4HI7XAY4vHX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DhimanTarafdar/AAA/blob/main/final_weakly_summery_ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Explain gradient descent in detail. What is the main idea behind it? How does it help in minimizing the loss function? Explain the role of learning rate and describe what happens if the learning rate is too high or too low."
      ],
      "metadata": {
        "id": "Cl2AovO_Bdqo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "Gradient Descent হলো একটি মৌলিক optimization algorithm যা আমাদের দেখায় কীভাবে মডেলের parameter গুলো adjust করতে হয় যাতে error কমানো যায়। নিচে এটি কীভাবে কাজ করে তার বিস্তারিত ব্যাখ্যা দেওয়া হলো।\n",
        "\n",
        "### 1. The Main Idea Behind Gradient Descent\n",
        "মূল ধারণা হলো **Weights ($W$)** এবং **Bias ($b$)** এর সর্বোত্তম মান খুঁজে বের করা যাতে মডেলের prediction যতটা সম্ভব সঠিক হয়। এটি একটি কম্পাসের মতো কাজ করে যা বলে দেয় কোন দিকে গেলে error সবচেয়ে কম হবে।\n",
        "\n",
        "* **The Process:** এটি বর্তমান অবস্থানে loss function এর gradient (slope) হিসাব করে।\n",
        "* **Direction:** যদি slope ধনাত্মক (positive) হয়, তাহলে আমরা বিপরীত দিকে যাই। যদি ঋণাত্মক (negative) হয়, তাহলে সামনে এগোই।\n",
        "* **Goal:** আমরা বারবার weight update করি যতক্ষণ না **Global Minimum** এ পৌঁছাই, যেখানে loss সর্বনিম্ন হয় এবং আমরা সর্বোত্তম class separation পাই।\n",
        "\n",
        "\n",
        "\n",
        "### 2. Minimizing the Loss Function (Mathematical Intuition)\n",
        "আমাদের মডেল কতটা ভুল করছে তা নির্ণয় করার জন্য আমরা **Mean Squared Error (MSE)** loss function ব্যবহার করি। একটি single prediction এর জন্য loss ($L$) হলো:\n",
        "\n",
        "$$L = (y - \\hat{y})^2$$\n",
        "\n",
        "যেহেতু $\\hat{y} = W \\cdot X + b$, তাই আমরা দেখতে পাচ্ছি loss আমাদের weight এর উপর নির্ভরশীল। এটিকে minimize করার জন্য আমরা loss কে weight ($W$) এর প্রতি partial derivative করি:\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial W} = \\frac{\\partial}{\\partial W} (y - (W \\cdot X + b))^2$$\n",
        "$$\\frac{\\partial L}{\\partial W} = 2(y - \\hat{y}) \\cdot (-X)$$\n",
        "\n",
        "**How it helps:**\n",
        "Derivative $\\frac{\\partial L}{\\partial W}$ আমাদের error hill এর \"slope\" জানায়। যখন error বেশি থাকে, তখন derivative বড় হয়, ফলে update বড় হয়। আর যখন আমরা সঠিক weight এর কাছাকাছি যাই, তখন $\\frac{\\partial L}{\\partial W}$ খুব ছোট হয়ে যায় (শূন্যের দিকে ধাবিত হয়)। এর ফলে মডেল ধীরে ধীরে স্থির হয়ে সর্বোত্তম মানে পৌঁছায়।\n",
        "\n",
        "### 3. The Weight Update Rule\n",
        "প্রতিটি iteration এ আমরা নিচের সূত্র ব্যবহার করে weight update করি:\n",
        "\n",
        "$$W_{new} = W_{old} - \\eta \\cdot \\frac{\\partial L}{\\partial W}$$\n",
        "\n",
        "এখানে $\\eta$ (Eta) হলো **Learning Rate**।\n",
        "\n",
        "### 4. The Role of Learning Rate ($\\eta$)\n",
        "Learning rate সবচেয়ে গুরুত্বপূর্ণ setting কারণ এটি নির্ধারণ করে আমরা minimum এর দিকে কত বড় \"step\" নেব।\n",
        "\n",
        "* **If the Learning Rate is too Small:**\n",
        "    * মডেল খুব ছোট ছোট step নেয়।\n",
        "    * **Result:** Training খুব ধীরে হয় এবং সিদ্ধান্তে পৌঁছাতে অনেক সময় লাগে। এমনকি global minimum এ পৌঁছানোর আগেই আটকে যেতে পারে।\n",
        "  \n",
        "* **If the Learning Rate is too High:**\n",
        "    * মডেল খুব বড় \"jump\" নেয়।\n",
        "    * **Result:** এটি global minimum এর উপর দিয়ে লাফিয়ে চলে যেতে পারে। সর্বোত্তম পয়েন্টে স্থির হওয়ার বদলে সামনে-পেছনে bounce করতে থাকে এবং loss বাড়তেও পারে।\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "OpvAttnTBlya"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kX8UlML5BfwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. What is forward propagation in an artificial neural network? Explain step by step how data moves from the input layer to the output layer."
      ],
      "metadata": {
        "id": "KE8k3cFOBy7p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "Forward propagation হলো সেই প্রক্রিয়া যার মাধ্যমে input data নেটওয়ার্কের বিভিন্ন layer অতিক্রম করে একটি prediction তৈরি করে। এটি হলো একটি neural network কীভাবে \"চিন্তা\" করে এবং input থেকে output ম্যাপ করে তার মৌলিক উপায়।\n",
        "\n",
        "### 1. The Core Concept\n",
        "একটি Artificial Neural Network (ANN) এ forward propagation মূলত একাধিক linear algebra operation এবং তার পরে non-linear transformation এর সমষ্টি। Data input layer থেকে শুরু করে hidden layer গুলো অতিক্রম করে শেষ পর্যন্ত output layer এ পৌঁছে আমাদের চূড়ান্ত ফলাফল ($\\hat{y}$) প্রদান করে।\n",
        "\n",
        "### 2. Step-by-Step Data Flow\n",
        "আমরা layer গুলোর মধ্যে data চলাচলকে নিচেরভাবে ব্যাখ্যা করতে পারি:\n",
        "\n",
        "* **From Input Layer ($a^{[0]}$) to Hidden Layer 1 ($a^{[1]}$):**\n",
        "    Input feature ($X$ বা $a^{[0]}$) weight ($W^{[1]}$) দিয়ে গুণ করা হয় এবং তার সাথে bias ($b^{[1]}$) যোগ করা হয়। এরপর এই ফলাফলকে একটি activation function যেমন **Sigmoid** ($\\sigma$) এর মাধ্যমে পাঠানো হয় যাতে non-linearity যুক্ত হয়।\n",
        "    $$z^{[1]} = W^{[1]} \\cdot a^{[0]} + b^{[1]}$$\n",
        "    $$a^{[1]} = \\sigma(z^{[1]})$$\n",
        "\n",
        "* **From Hidden Layer 1 ($a^{[1]}$) to Hidden Layer 2 ($a^{[2]}$):**\n",
        "    প্রথম hidden layer এর output দ্বিতীয় layer এর input হিসেবে কাজ করে।\n",
        "    $$z^{[2]} = W^{[2]} \\cdot a^{[1]} + b^{[2]}$$\n",
        "    $$a^{[2]} = \\sigma(z^{[2]})$$\n",
        "\n",
        "* **Generating the Final Output ($\\hat{y}$):**\n",
        "    এভাবে data output layer পর্যন্ত পৌঁছায়। যদি আমাদের 3টি layer থাকে, তাহলে চূড়ান্ত prediction ($\\hat{y}$) বা $a^{[3]}$ হিসাব করা হয়।\n",
        "\n",
        "\n",
        "\n",
        "### 3. The General Equation (Nested View)\n",
        "পুরো প্রক্রিয়াটি গাণিতিকভাবে দেখলে এটি একটি nested function এর মতো। একটি 3-layer network এর জন্য সামগ্রিক গণনাটি হবে:\n",
        "\n",
        "$$\\hat{y} = \\sigma(W^{[3]} \\cdot \\sigma(W^{[2]} \\cdot \\sigma(W^{[1]} \\cdot X + b^{[1]}) + b^{[2]}) + b^{[3]})$$\n",
        "\n",
        "### 4. Role of the Sigmoid Function\n",
        "Forward propagation চলাকালে **Sigmoid Function** অত্যন্ত গুরুত্বপূর্ণ ভূমিকা পালন করে।\n",
        "* **Non-linearity:** এটি না থাকলে আমাদের মডেল একটি সাধারণ linear regressor এ পরিণত হতো।\n",
        "* **Curved Boundaries:** যেমন আমি লক্ষ্য করেছি, sigmoid function মডেলকে decision boundary বাঁকাতে (bend) সাহায্য করে। এর ফলে curved boundary তৈরি হয়, যা জটিল pattern classify করতে সক্ষম যেখানে একটি সরল রেখা ব্যর্থ হয়।\n",
        "* **Output Range:** এটি যেকোনো input মানকে 0 থেকে 1 এর মধ্যে সীমাবদ্ধ (squash) করে, যা probability ভিত্তিক classification এর জন্য উপযুক্ত।\n",
        "\n",
        "\n",
        "\n",
        "### 5. Intuition\n",
        "প্রতিটি layer এর neuron পূর্বের layer থেকে signal গ্রহণ করে, একটি weighted sum গণনা করে এবং activation function এর মাধ্যমে সিদ্ধান্ত নেয় কতটুকু signal সামনে পাঠাবে। এইভাবে layer-by-layer তথ্য আদান-প্রদান করার প্রক্রিয়াকেই আমরা Forward Propagation বলি।\n",
        "\n",
        "---\n",
        "```\n"
      ],
      "metadata": {
        "id": "CDb4BDhvCQ-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Why do we need activation functions in neural networks? What would happen if we only used linear transformations without any activation function?"
      ],
      "metadata": {
        "id": "02LSfw4MCdMg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "Activation function হলো সেই গোপন উপাদান যা Deep Learning-কে সত্যিকার অর্থে \"deep\" বানায়। এগুলো না থাকলে একটি neural network কেবলমাত্র একটি উন্নত (glorified) linear regression model হয়ে যেত। নিচে ব্যাখ্যা করা হলো কেন এগুলো অপরিহার্য।\n",
        "\n",
        "### 1. Breaking Linearity (The Core Reason)\n",
        "Activation function এর প্রধান কাজ হলো নেটওয়ার্কে **non-linearity** যুক্ত করা। বাস্তব জগতের বেশিরভাগ data (যেমন image, audio, বা medical record) জটিল এবং একটি সাধারণ সরল রেখা দিয়ে আলাদা করা যায় না।\n",
        "\n",
        "* **Linear Transformation:** প্রতিটি layer গণনা করে $Z = W \\cdot X + b$। এটি কেবল একটি সরল রেখা নির্দেশ করে।\n",
        "* **Non-Linear Transformation:** যখন আমরা $\\sigma(Z)$ বা $ReLU(Z)$ এর মতো activation function প্রয়োগ করি, তখন আমরা সেই সরল রেখাটিকে \"bend\" বা বাঁকাই। এর ফলে মডেল জটিল ও বক্র (curved) decision boundary তৈরি করতে পারে, যা কঠিন pattern গুলোকে classify করতে সক্ষম।\n",
        "\n",
        "\n",
        "\n",
        "### 2. What Happens Without Activation Functions? (Mathematical Collapse)\n",
        "যদি আমরা activation function ছাড়া একাধিক layer একত্রে stack করি, তাহলে পুরো network একটি single linear layer এ \"collapse\" হয়ে যায়।\n",
        "\n",
        "ধরি আমাদের দুটি layer আছে:\n",
        "1. $h_1 = W_1 \\cdot X + b_1$\n",
        "2. $y = W_2 \\cdot h_1 + b_2$\n",
        "\n",
        "এখন $h_1$ কে দ্বিতীয় সমীকরণে বসাই:\n",
        "$$y = W_2 \\cdot (W_1 \\cdot X + b_1) + b_2$$\n",
        "$$y = (W_2 \\cdot W_1) \\cdot X + (W_2 \\cdot b_1 + b_2)$$\n",
        "\n",
        "গাণিতিকভাবে, $(W_2 \\cdot W_1)$ হলো আরেকটি matrix ($W_{new}$) এবং $(W_2 \\cdot b_1 + b_2)$ হলো আরেকটি bias ($b_{new}$)।  \n",
        "অতএব, ফলাফল দাঁড়ায়:\n",
        "$$y = W_{new} \\cdot X + b_{new}$$\n",
        "\n",
        "**The Outcome:** আমরা যতগুলোই hidden layer যোগ করি (10, 100, বা 1000), মডেলটি শেষ পর্যন্ত একটি **Single Layer Perceptron** হিসেবেই কাজ করবে। Activation function ব্যবহার না করলে network \"deep\" করলেও অতিরিক্ত learning power পাওয়া যাবে না।\n",
        "\n",
        "### 3. A Simple Intuition (The Glass Layer Analogy)\n",
        "আমি এটি একটি উপমা দিয়ে কল্পনা করতে পছন্দ করি:  \n",
        "> ভাবুন আপনার কাছে কয়েকটি স্বচ্ছ কাচের শিট আছে। আপনি যদি প্রতিটি কাচে একটি করে সরল রেখা আঁকেন এবং সেগুলো একটির উপর আরেকটি ঠিকভাবে রাখেন, তাহলে উপরে থেকে দেখলে আপনি এখনও শুধু একটি **সরল রেখা**-ই দেখতে পাবেন। কখনোই কেবল সরল রেখা স্তূপ করে একটি বৃত্ত, বক্ররেখা বা জটিল মানচিত্র তৈরি করা সম্ভব নয়।\n",
        "\n",
        "Activation function একটি lens এর মতো কাজ করে যা সেই সরল রেখাগুলোকে বাঁকায় ও বিকৃত করে। প্রতিটি layer এ রেখাগুলো বাঁকানোর মাধ্যমে আমরা ধীরে ধীরে জটিল আকার ও pattern (যেমন বৃত্ত বা zig-zag) তৈরি করতে পারি, যা আমাদের data সঠিকভাবে আলাদা করতে সাহায্য করে।\n",
        "\n",
        "\n",
        "\n",
        "### 4. Helping Gradient Descent (Derivatives)\n",
        "Activation function training প্রক্রিয়াতেও গুরুত্বপূর্ণ ভূমিকা রাখে। Gradient Descent কাজ করার জন্য আমাদের derivative (gradient) গণনা করতে হয়।  \n",
        "* **Sigmoid** বা **Tanh** এর মতো function প্রতিটি পয়েন্টে differentiable।  \n",
        "* এর ফলে মডেল বুঝতে পারে weight সামান্য পরিবর্তন করলে error কতটা পরিবর্তিত হয়। উপযুক্ত activation function ছাড়া multi-layer context এ gradient গণনা অর্থহীন হয়ে পড়ে।\n",
        "\n",
        "### Summary of Why We Need Them:\n",
        "1. **To Solve Non-Linear Problems:** বাস্তব data সাধারণত linear নয়।\n",
        "2. **To Prevent Layer Collapse:** যাতে \"Deep\" সত্যিই বেশি learning capacity বোঝায়।\n",
        "3. **To Enable Backpropagation:** Gradient গণনা ও weight update করার সুযোগ প্রদান করে।\n",
        "\n",
        "---\n",
        "```\n"
      ],
      "metadata": {
        "id": "kmyx7wDbCiBA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a9nEaAP_B3aF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}