{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1GDlD0n3D5Q4Woj0o5TC1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DhimanTarafdar/AAA/blob/main/final_weakly_summery_ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Explain gradient descent in detail. What is the main idea behind it? How does it help in minimizing the loss function? Explain the role of learning rate and describe what happens if the learning rate is too high or too low."
      ],
      "metadata": {
        "id": "Cl2AovO_Bdqo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "Gradient Descent হলো একটি মৌলিক optimization algorithm যা আমাদের দেখায় কীভাবে মডেলের parameter গুলো adjust করতে হয় যাতে error কমানো যায়। নিচে এটি কীভাবে কাজ করে তার বিস্তারিত ব্যাখ্যা দেওয়া হলো।\n",
        "\n",
        "### 1. The Main Idea Behind Gradient Descent\n",
        "মূল ধারণা হলো **Weights ($W$)** এবং **Bias ($b$)** এর সর্বোত্তম মান খুঁজে বের করা যাতে মডেলের prediction যতটা সম্ভব সঠিক হয়। এটি একটি কম্পাসের মতো কাজ করে যা বলে দেয় কোন দিকে গেলে error সবচেয়ে কম হবে।\n",
        "\n",
        "* **The Process:** এটি বর্তমান অবস্থানে loss function এর gradient (slope) হিসাব করে।\n",
        "* **Direction:** যদি slope ধনাত্মক (positive) হয়, তাহলে আমরা বিপরীত দিকে যাই। যদি ঋণাত্মক (negative) হয়, তাহলে সামনে এগোই।\n",
        "* **Goal:** আমরা বারবার weight update করি যতক্ষণ না **Global Minimum** এ পৌঁছাই, যেখানে loss সর্বনিম্ন হয় এবং আমরা সর্বোত্তম class separation পাই।\n",
        "\n",
        "\n",
        "\n",
        "### 2. Minimizing the Loss Function (Mathematical Intuition)\n",
        "আমাদের মডেল কতটা ভুল করছে তা নির্ণয় করার জন্য আমরা **Mean Squared Error (MSE)** loss function ব্যবহার করি। একটি single prediction এর জন্য loss ($L$) হলো:\n",
        "\n",
        "$$L = (y - \\hat{y})^2$$\n",
        "\n",
        "যেহেতু $\\hat{y} = W \\cdot X + b$, তাই আমরা দেখতে পাচ্ছি loss আমাদের weight এর উপর নির্ভরশীল। এটিকে minimize করার জন্য আমরা loss কে weight ($W$) এর প্রতি partial derivative করি:\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial W} = \\frac{\\partial}{\\partial W} (y - (W \\cdot X + b))^2$$\n",
        "$$\\frac{\\partial L}{\\partial W} = 2(y - \\hat{y}) \\cdot (-X)$$\n",
        "\n",
        "**How it helps:**\n",
        "Derivative $\\frac{\\partial L}{\\partial W}$ আমাদের error hill এর \"slope\" জানায়। যখন error বেশি থাকে, তখন derivative বড় হয়, ফলে update বড় হয়। আর যখন আমরা সঠিক weight এর কাছাকাছি যাই, তখন $\\frac{\\partial L}{\\partial W}$ খুব ছোট হয়ে যায় (শূন্যের দিকে ধাবিত হয়)। এর ফলে মডেল ধীরে ধীরে স্থির হয়ে সর্বোত্তম মানে পৌঁছায়।\n",
        "\n",
        "### 3. The Weight Update Rule\n",
        "প্রতিটি iteration এ আমরা নিচের সূত্র ব্যবহার করে weight update করি:\n",
        "\n",
        "$$W_{new} = W_{old} - \\eta \\cdot \\frac{\\partial L}{\\partial W}$$\n",
        "\n",
        "এখানে $\\eta$ (Eta) হলো **Learning Rate**।\n",
        "\n",
        "### 4. The Role of Learning Rate ($\\eta$)\n",
        "Learning rate সবচেয়ে গুরুত্বপূর্ণ setting কারণ এটি নির্ধারণ করে আমরা minimum এর দিকে কত বড় \"step\" নেব।\n",
        "\n",
        "* **If the Learning Rate is too Small:**\n",
        "    * মডেল খুব ছোট ছোট step নেয়।\n",
        "    * **Result:** Training খুব ধীরে হয় এবং সিদ্ধান্তে পৌঁছাতে অনেক সময় লাগে। এমনকি global minimum এ পৌঁছানোর আগেই আটকে যেতে পারে।\n",
        "  \n",
        "* **If the Learning Rate is too High:**\n",
        "    * মডেল খুব বড় \"jump\" নেয়।\n",
        "    * **Result:** এটি global minimum এর উপর দিয়ে লাফিয়ে চলে যেতে পারে। সর্বোত্তম পয়েন্টে স্থির হওয়ার বদলে সামনে-পেছনে bounce করতে থাকে এবং loss বাড়তেও পারে।\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "OpvAttnTBlya"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kX8UlML5BfwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. What is forward propagation in an artificial neural network? Explain step by step how data moves from the input layer to the output layer."
      ],
      "metadata": {
        "id": "KE8k3cFOBy7p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "Forward propagation হলো সেই প্রক্রিয়া যার মাধ্যমে input data নেটওয়ার্কের বিভিন্ন layer অতিক্রম করে একটি prediction তৈরি করে। এটি হলো একটি neural network কীভাবে \"চিন্তা\" করে এবং input থেকে output ম্যাপ করে তার মৌলিক উপায়।\n",
        "\n",
        "### 1. The Core Concept\n",
        "একটি Artificial Neural Network (ANN) এ forward propagation মূলত একাধিক linear algebra operation এবং তার পরে non-linear transformation এর সমষ্টি। Data input layer থেকে শুরু করে hidden layer গুলো অতিক্রম করে শেষ পর্যন্ত output layer এ পৌঁছে আমাদের চূড়ান্ত ফলাফল ($\\hat{y}$) প্রদান করে।\n",
        "\n",
        "### 2. Step-by-Step Data Flow\n",
        "আমরা layer গুলোর মধ্যে data চলাচলকে নিচেরভাবে ব্যাখ্যা করতে পারি:\n",
        "\n",
        "* **From Input Layer ($a^{[0]}$) to Hidden Layer 1 ($a^{[1]}$):**\n",
        "    Input feature ($X$ বা $a^{[0]}$) weight ($W^{[1]}$) দিয়ে গুণ করা হয় এবং তার সাথে bias ($b^{[1]}$) যোগ করা হয়। এরপর এই ফলাফলকে একটি activation function যেমন **Sigmoid** ($\\sigma$) এর মাধ্যমে পাঠানো হয় যাতে non-linearity যুক্ত হয়।\n",
        "    $$z^{[1]} = W^{[1]} \\cdot a^{[0]} + b^{[1]}$$\n",
        "    $$a^{[1]} = \\sigma(z^{[1]})$$\n",
        "\n",
        "* **From Hidden Layer 1 ($a^{[1]}$) to Hidden Layer 2 ($a^{[2]}$):**\n",
        "    প্রথম hidden layer এর output দ্বিতীয় layer এর input হিসেবে কাজ করে।\n",
        "    $$z^{[2]} = W^{[2]} \\cdot a^{[1]} + b^{[2]}$$\n",
        "    $$a^{[2]} = \\sigma(z^{[2]})$$\n",
        "\n",
        "* **Generating the Final Output ($\\hat{y}$):**\n",
        "    এভাবে data output layer পর্যন্ত পৌঁছায়। যদি আমাদের 3টি layer থাকে, তাহলে চূড়ান্ত prediction ($\\hat{y}$) বা $a^{[3]}$ হিসাব করা হয়।\n",
        "\n",
        "\n",
        "\n",
        "### 3. The General Equation (Nested View)\n",
        "পুরো প্রক্রিয়াটি গাণিতিকভাবে দেখলে এটি একটি nested function এর মতো। একটি 3-layer network এর জন্য সামগ্রিক গণনাটি হবে:\n",
        "\n",
        "$$\\hat{y} = \\sigma(W^{[3]} \\cdot \\sigma(W^{[2]} \\cdot \\sigma(W^{[1]} \\cdot X + b^{[1]}) + b^{[2]}) + b^{[3]})$$\n",
        "\n",
        "### 4. Role of the Sigmoid Function\n",
        "Forward propagation চলাকালে **Sigmoid Function** অত্যন্ত গুরুত্বপূর্ণ ভূমিকা পালন করে।\n",
        "* **Non-linearity:** এটি না থাকলে আমাদের মডেল একটি সাধারণ linear regressor এ পরিণত হতো।\n",
        "* **Curved Boundaries:** যেমন আমি লক্ষ্য করেছি, sigmoid function মডেলকে decision boundary বাঁকাতে (bend) সাহায্য করে। এর ফলে curved boundary তৈরি হয়, যা জটিল pattern classify করতে সক্ষম যেখানে একটি সরল রেখা ব্যর্থ হয়।\n",
        "* **Output Range:** এটি যেকোনো input মানকে 0 থেকে 1 এর মধ্যে সীমাবদ্ধ (squash) করে, যা probability ভিত্তিক classification এর জন্য উপযুক্ত।\n",
        "\n",
        "\n",
        "\n",
        "### 5. Intuition\n",
        "প্রতিটি layer এর neuron পূর্বের layer থেকে signal গ্রহণ করে, একটি weighted sum গণনা করে এবং activation function এর মাধ্যমে সিদ্ধান্ত নেয় কতটুকু signal সামনে পাঠাবে। এইভাবে layer-by-layer তথ্য আদান-প্রদান করার প্রক্রিয়াকেই আমরা Forward Propagation বলি।\n",
        "\n",
        "---\n",
        "```\n"
      ],
      "metadata": {
        "id": "CDb4BDhvCQ-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Why do we need activation functions in neural networks? What would happen if we only used linear transformations without any activation function?"
      ],
      "metadata": {
        "id": "02LSfw4MCdMg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "Activation function হলো সেই গোপন উপাদান যা Deep Learning-কে সত্যিকার অর্থে \"deep\" বানায়। এগুলো না থাকলে একটি neural network কেবলমাত্র একটি উন্নত (glorified) linear regression model হয়ে যেত। নিচে ব্যাখ্যা করা হলো কেন এগুলো অপরিহার্য।\n",
        "\n",
        "### 1. Breaking Linearity (The Core Reason)\n",
        "Activation function এর প্রধান কাজ হলো নেটওয়ার্কে **non-linearity** যুক্ত করা। বাস্তব জগতের বেশিরভাগ data (যেমন image, audio, বা medical record) জটিল এবং একটি সাধারণ সরল রেখা দিয়ে আলাদা করা যায় না।\n",
        "\n",
        "* **Linear Transformation:** প্রতিটি layer গণনা করে $Z = W \\cdot X + b$। এটি কেবল একটি সরল রেখা নির্দেশ করে।\n",
        "* **Non-Linear Transformation:** যখন আমরা $\\sigma(Z)$ বা $ReLU(Z)$ এর মতো activation function প্রয়োগ করি, তখন আমরা সেই সরল রেখাটিকে \"bend\" বা বাঁকাই। এর ফলে মডেল জটিল ও বক্র (curved) decision boundary তৈরি করতে পারে, যা কঠিন pattern গুলোকে classify করতে সক্ষম।\n",
        "\n",
        "\n",
        "\n",
        "### 2. What Happens Without Activation Functions? (Mathematical Collapse)\n",
        "যদি আমরা activation function ছাড়া একাধিক layer একত্রে stack করি, তাহলে পুরো network একটি single linear layer এ \"collapse\" হয়ে যায়।\n",
        "\n",
        "ধরি আমাদের দুটি layer আছে:\n",
        "1. $h_1 = W_1 \\cdot X + b_1$\n",
        "2. $y = W_2 \\cdot h_1 + b_2$\n",
        "\n",
        "এখন $h_1$ কে দ্বিতীয় সমীকরণে বসাই:\n",
        "$$y = W_2 \\cdot (W_1 \\cdot X + b_1) + b_2$$\n",
        "$$y = (W_2 \\cdot W_1) \\cdot X + (W_2 \\cdot b_1 + b_2)$$\n",
        "\n",
        "গাণিতিকভাবে, $(W_2 \\cdot W_1)$ হলো আরেকটি matrix ($W_{new}$) এবং $(W_2 \\cdot b_1 + b_2)$ হলো আরেকটি bias ($b_{new}$)।  \n",
        "অতএব, ফলাফল দাঁড়ায়:\n",
        "$$y = W_{new} \\cdot X + b_{new}$$\n",
        "\n",
        "**The Outcome:** আমরা যতগুলোই hidden layer যোগ করি (10, 100, বা 1000), মডেলটি শেষ পর্যন্ত একটি **Single Layer Perceptron** হিসেবেই কাজ করবে। Activation function ব্যবহার না করলে network \"deep\" করলেও অতিরিক্ত learning power পাওয়া যাবে না।\n",
        "\n",
        "### 3. A Simple Intuition (The Glass Layer Analogy)\n",
        "আমি এটি একটি উপমা দিয়ে কল্পনা করতে পছন্দ করি:  \n",
        "> ভাবুন আপনার কাছে কয়েকটি স্বচ্ছ কাচের শিট আছে। আপনি যদি প্রতিটি কাচে একটি করে সরল রেখা আঁকেন এবং সেগুলো একটির উপর আরেকটি ঠিকভাবে রাখেন, তাহলে উপরে থেকে দেখলে আপনি এখনও শুধু একটি **সরল রেখা**-ই দেখতে পাবেন। কখনোই কেবল সরল রেখা স্তূপ করে একটি বৃত্ত, বক্ররেখা বা জটিল মানচিত্র তৈরি করা সম্ভব নয়।\n",
        "\n",
        "Activation function একটি lens এর মতো কাজ করে যা সেই সরল রেখাগুলোকে বাঁকায় ও বিকৃত করে। প্রতিটি layer এ রেখাগুলো বাঁকানোর মাধ্যমে আমরা ধীরে ধীরে জটিল আকার ও pattern (যেমন বৃত্ত বা zig-zag) তৈরি করতে পারি, যা আমাদের data সঠিকভাবে আলাদা করতে সাহায্য করে।\n",
        "\n",
        "\n",
        "\n",
        "### 4. Helping Gradient Descent (Derivatives)\n",
        "Activation function training প্রক্রিয়াতেও গুরুত্বপূর্ণ ভূমিকা রাখে। Gradient Descent কাজ করার জন্য আমাদের derivative (gradient) গণনা করতে হয়।  \n",
        "* **Sigmoid** বা **Tanh** এর মতো function প্রতিটি পয়েন্টে differentiable।  \n",
        "* এর ফলে মডেল বুঝতে পারে weight সামান্য পরিবর্তন করলে error কতটা পরিবর্তিত হয়। উপযুক্ত activation function ছাড়া multi-layer context এ gradient গণনা অর্থহীন হয়ে পড়ে।\n",
        "\n",
        "### Summary of Why We Need Them:\n",
        "1. **To Solve Non-Linear Problems:** বাস্তব data সাধারণত linear নয়।\n",
        "2. **To Prevent Layer Collapse:** যাতে \"Deep\" সত্যিই বেশি learning capacity বোঝায়।\n",
        "3. **To Enable Backpropagation:** Gradient গণনা ও weight update করার সুযোগ প্রদান করে।\n",
        "\n",
        "---\n",
        "```\n"
      ],
      "metadata": {
        "id": "kmyx7wDbCiBA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Explain the sigmoid activation function. Where is it used? What are its advantages and disadvantages? What problem does it cause during training?"
      ],
      "metadata": {
        "id": "k1l9qIbdCtQL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "Sigmoid activation function Deep Learning-এর অন্যতম প্রাচীন ও ক্লাসিক activation function। এটি একটি non-linear function যা input মানকে একটি নির্দিষ্ট probability range-এর মধ্যে সীমাবদ্ধ (squash) করে।\n",
        "\n",
        "### 1. What is the Sigmoid Function?\n",
        "গাণিতিকভাবে, Sigmoid function যেকোনো real-valued সংখ্যা কে **0 এবং 1** এর মধ্যে map করে। Sigmoid function-এর সূত্র হলো:\n",
        "\n",
        "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
        "\n",
        "\n",
        "\n",
        "### 2. How it Helps in Classification\n",
        "Sigmoid function-এর output কে একটি **probability** হিসেবে ব্যাখ্যা করা যায়।  \n",
        "* **Range:** যেহেতু output সবসময় 0 এবং 1 এর মধ্যে থাকে, তাই এটি সেইসব মডেলের জন্য উপযুক্ত যেখানে একটি class-এর probability predict করতে হয়।  \n",
        "* **Threshold:** সাধারণত আমরা **0.5** threshold ব্যবহার করি। যদি output $\\geq 0.5$ হয়, তাহলে এটিকে Class 1 হিসেবে classify করি; আর যদি $< 0.5$ হয়, তাহলে Class 0 হিসেবে ধরি। এর ফলে সহজেই একটি decision boundary তৈরি করা যায় এবং বিভিন্ন class আলাদা করা সম্ভব হয়।\n",
        "\n",
        "### 3. Where is it Used?\n",
        "* **Output Layer:** এটি প্রধানত **Binary Classification** সমস্যার output layer-এ ব্যবহার করা হয় (যেমন: একটি email Spam কিনা বা Not Spam তা predict করা)।  \n",
        "* **Early Neural Networks:** ঐতিহাসিকভাবে hidden layer-এও ব্যবহার হতো, যদিও আধুনিক architecture-এ এটি বেশিরভাগ ক্ষেত্রে ReLU দ্বারা প্রতিস্থাপিত হয়েছে।\n",
        "\n",
        "### 4. Advantages and Disadvantages\n",
        "| Advantages | Disadvantages |\n",
        "| :--- | :--- |\n",
        "| **Smooth Gradient:** এর curve মসৃণ, তাই যেকোনো point-এ সহজেই derivative গণনা করা যায়। | **Vanishing Gradient Problem:** Training চলাকালে এটি সবচেয়ে বড় সমস্যা। |\n",
        "| **Probability Output:** Classification task-এর জন্য পরিষ্কার probability interpretation দেয়। | **Computationally Expensive:** Exponential function ($e^{-z}$) গণনা করা ReLU-এর তুলনায় ধীর। |\n",
        "| **Normalization:** Activation-কে খুব বেশি বড় মানে explode করতে দেয় না। | **Not Zero-Centered:** Output সবসময় positive হওয়ায় gradient update কম কার্যকর হতে পারে। |\n",
        "\n",
        "### 5. The Problem During Training: Vanishing Gradient\n",
        "Sigmoid-এর ক্ষেত্রে training চলাকালে সবচেয়ে গুরুত্বপূর্ণ সমস্যা হলো **Vanishing Gradient Problem**।\n",
        "\n",
        "* **The Cause:** Sigmoid curve-এর দুই প্রান্তে (যখন $z$ খুব বড় বা খুব ছোট) graph অনেকটা flat হয়ে যায়।  \n",
        "* **The Effect:** এই flat অঞ্চলে derivative (gradient) প্রায় **zero** হয়ে যায়।  \n",
        "* **The Result:** Backpropagation চলাকালে layer থেকে layer-এ এই ছোট gradient গুলো গুণ হতে থাকে। ফলে আগের layer-এ পৌঁছাতে পৌঁছাতে gradient এতটাই ছোট হয়ে যায় (vanish করে) যে weight আর update হয় না। কার্যত মডেল **শেখা বন্ধ করে দেয়**, এবং training plateau হয়ে যায়।\n",
        "\n",
        "---\n",
        "```\n"
      ],
      "metadata": {
        "id": "tRAPpcLBCxyU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. Explain the softmax activation function in simple words. Why is it used in multi-class classification problems? How is it different from sigmoid?"
      ],
      "metadata": {
        "id": "2WCBYHMVC6nq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "যখন আমাদের দুইটির বেশি category নিয়ে কাজ করতে হয়, তখন সবগুলো class-এর মধ্যে probability বণ্টন করার একটি উপায় প্রয়োজন। ঠিক এই কাজটিই **Softmax** activation function করে।\n",
        "\n",
        "### 1. What is Softmax?\n",
        "সহজ ভাষায়, Softmax হলো একটি activation function যা একটি সংখ্যার vector (logits) কে probability-এর vector-এ রূপান্তর করে। এই probability গুলোর যোগফল সবসময় **1** হয়।\n",
        "\n",
        "* **Logits ($Z$):** এগুলো হলো activation দেওয়ার আগে শেষ layer থেকে পাওয়া raw score।\n",
        "* **Probability:** Softmax নিশ্চিত করে যে প্রতিটি class-এর মান 0 এবং 1 এর মধ্যে থাকবে।\n",
        "\n",
        "নির্দিষ্ট একটি class $i$ এর জন্য Softmax-এর সূত্র হলো:\n",
        "$$\\sigma(Z_i) = \\frac{e^{Z_i}}{\\sum_{j=1}^{K} e^{Z_j}}$$\n",
        "\n",
        "এখানে, $K$ হলো মোট class-এর সংখ্যা। Exponential ($e$) ব্যবহার করার মাধ্যমে Softmax সবচেয়ে বড় logit-কে আরও বেশি highlight করে, অর্থাৎ সর্বোচ্চ মানকে উপরে ঠেলে দেয় এবং বাকিগুলোকে তুলনামূলকভাবে কমিয়ে দেয়।\n",
        "\n",
        "\n",
        "\n",
        "### 2. Use in Multi-class Classification\n",
        "ধরি আমরা একটি image classify করছি, যা হতে পারে **Cat, Dog, বা Horse**।  \n",
        "* মডেল তিনটি raw score (logits) তৈরি করে।  \n",
        "* Softmax সেই score গুলোকে রূপান্তর করে এমন কিছু দেয়: **Cat (0.70), Dog (0.20), Horse (0.10)**।  \n",
        "* যেহেতু 0.70 সবচেয়ে বেশি, তাই মডেল \"Cat\" predict করে।\n",
        "\n",
        "**Why it's great:** এটি শুধু বিজয়ী নির্ধারণ করে না; বরং প্রতিটি class সম্পর্কে মডেল কতটা confident তাও জানায়। যদি একটি class-এর probability বাড়ে, তাহলে অন্যগুলোর probability কমতেই হবে, কারণ মোট যোগফল সবসময় 1.0 থাকে।\n",
        "\n",
        "### 3. Softmax vs. Sigmoid: Key Differences\n",
        "যদিও দুটি function-ই probability নিয়ে কাজ করে, তবে এদের উদ্দেশ্য আলাদা:\n",
        "\n",
        "| Feature | Sigmoid | Softmax |\n",
        "| :--- | :--- | :--- |\n",
        "| **Output Type** | প্রতিটি class-এর জন্য স্বাধীন probability। | নির্ভরশীল probability (মোট যোগফল = 1)। |\n",
        "| **Best Used For** | **Binary Classification** (Yes/No, Spam/Not Spam)। | **Multi-class Classification** (Cat/Dog/Horse)। |\n",
        "| **Nature** | প্রতিটি output আলাদাভাবে বিবেচনা করে। | সব output-কে একটি সম্পর্কিত সেট হিসেবে বিবেচনা করে। |\n",
        "| **Outcome** | একাধিক class-এর probability বেশি হতে পারে। | সাধারণত একটি স্পষ্ট \"winner\" নির্ধারণ করে। |\n",
        "\n",
        "\n",
        "\n",
        "### 4. Summary of Mechanics\n",
        "Softmax হলো multi-class প্রতিযোগিতার একজন \"judge\"। Denominator-এ summation ব্যবহার করার মাধ্যমে এটি class গুলোর মধ্যে প্রতিযোগিতা তৈরি করে। যেমন আমি লক্ষ্য করেছি, যদি \"Cat\"-এর logit অন্যগুলোর তুলনায় সামান্য বেশি হয়, তাহলে Softmax সেই পার্থক্যকে বাড়িয়ে তোলে, ফলে চূড়ান্ত classification আরও পরিষ্কার ও নির্দিষ্ট হয়ে যায়।\n",
        "\n",
        "---\n",
        "```\n"
      ],
      "metadata": {
        "id": "jE38zSJSC_BB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a9nEaAP_B3aF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}